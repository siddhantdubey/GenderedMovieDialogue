{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score, validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_test = []\n",
    "Y_train = []\n",
    "Y_test = []\n",
    "X_valid = []\n",
    "Y_valid = []\n",
    "male = []\n",
    "female = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'data/training_set.txt') as training:\n",
    "    for line in training:\n",
    "        line = line.strip()\n",
    "        lnum, chr_id, movie_id, chr_name, chr_gender, line_text, credit_list = line.split(\"+++$+++\")\n",
    "        chr_gender = chr_gender.strip()\n",
    "#         ss = sid.polarity_scores(line_text)\n",
    "#         sentiment = ss['compound']\n",
    "        if(chr_gender.lower() == \"m\"):\n",
    "            X_train.append(line_text)\n",
    "            male.append(line_text)\n",
    "            Y_train.append(0)\n",
    "        elif(chr_gender.lower() == \"f\"):\n",
    "            X_train.append(line_text)\n",
    "            female.append(line_text)\n",
    "            Y_train.append(1)\n",
    "\n",
    "\n",
    "with open(r'data/test_set.txt') as test:\n",
    "    for line in test:\n",
    "        line = line.strip()\n",
    "        lnum, chr_id, movie_id, chr_name, chr_gender, line_text, credit_list = line.split(\"+++$+++\")\n",
    "        chr_gender = chr_gender.strip()\n",
    "#         ss = sid.polarity_scores(line_text)\n",
    "#         sentiment = ss['compound']\n",
    "        if(chr_gender.lower() == \"m\"):\n",
    "            X_test.append(line_text)\n",
    "            male.append(line_text)\n",
    "            Y_test.append(0)\n",
    "        elif(chr_gender.lower() == \"f\"):\n",
    "            X_test.append(line_text)\n",
    "            female.append(line_text)\n",
    "            Y_test.append(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(r'data/validation_set.txt') as valid:\n",
    "    for line in valid:\n",
    "        line = line.strip()\n",
    "        lnum, chr_id, movie_id, chr_name, chr_gender, line_text, credit_list = line.split(\"+++$+++\")\n",
    "        chr_gender = chr_gender.strip()\n",
    "#         ss = sid.polarity_scores(line_text)\n",
    "#         sentiment = ss['compound']\n",
    "        if(chr_gender.lower() == \"m\"):\n",
    "            X_valid.append(line_text)\n",
    "            male.append(line_text)\n",
    "            Y_valid.append(0)\n",
    "        elif(chr_gender.lower() == \"f\"):\n",
    "            X_valid.append(line_text)\n",
    "            female.append(line_text)\n",
    "            Y_valid.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train + X_test + X_valid\n",
    "Y = Y_train + Y_test + Y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(len(X)):\n",
    "  if(len(X[i]) > 0):\n",
    "    answer = True \n",
    "  else:\n",
    "    indices.append(i)\n",
    "\n",
    "for index in sorted(indices, reverse=True):\n",
    "    del X[index]\n",
    "    del Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X, Y = oversample.fit_resample(np.array(X).reshape(-1,1), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for phrase in X:\n",
    "    a.append(phrase[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = a\n",
    "print(len(X))\n",
    "print(type(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['text'] = X\n",
    "df['target'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/tweets.csv', encoding=\"charmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df['text']\n",
    "test_target = test_df['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test_data = []\n",
    "clean_test_target = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_target)):\n",
    "    element = test_target[i]\n",
    "    text = test_data[i]\n",
    "    if element == \"male\":\n",
    "        clean_test_data.append(text)\n",
    "        clean_test_target.append(0)\n",
    "    elif element == \"female\":\n",
    "        clean_test_data.append(text)\n",
    "        clean_test_target.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[\"text\"]\n",
    "train_target = df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = clean_test_data\n",
    "test_target = clean_test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"classifier\", MultinomialNB()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "bow_pipeline.fit(train_data, train_target)\n",
    "y_pred = bow_pipeline.predict(test_data)\n",
    "cr = classification_report(test_target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class SegmentFeaturizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.future_words = [\"tomorrow\", \"future\", \"futures\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_propernouns(doc):\n",
    "        segment = doc.text.lower().split()\n",
    "        count = 0 \n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.tag_ in ['NNP', 'NNPS']:\n",
    "                count+=1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if(count == 0):\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_words_before_main_verb(doc):\n",
    "        numbers = [0]\n",
    "        for sent in doc.sents:\n",
    "            main = [t for t in sent if t.dep_ == \"ROOT\"][0]\n",
    "            if main.pos_ == \"VERB\":\n",
    "                dist_to_init = main.i - sent[0].i\n",
    "                numbers.append(dist_to_init)\n",
    "        return np.mean(numbers)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_complex_clauses(doc):\n",
    "        embedded_elements_count = []\n",
    "        for sent in doc.sents:\n",
    "            n_embedded = len(\n",
    "                [t for t in sent if t.dep_ in {\"ccomp\", \"xcomp\", \"advcl\", \"dative\"}]\n",
    "            )\n",
    "            embedded_elements_count.append(n_embedded)\n",
    "        if len(embedded_elements_count) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.mean(embedded_elements_count)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mean_sentiment(doc):\n",
    "        return doc.sentiment\n",
    "    @staticmethod\n",
    "    def get_pronouns(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PRON\":\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    @staticmethod\n",
    "    def get_female_pronouns(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.text.lower() in ['her', 'she', 'wife', 'girlfriend']:\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num+=1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    @staticmethod\n",
    "    def get_male_pronouns(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.text.lower() in ['he', 'him', 'husband', 'honey', 'his', 'boyfriend']:\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    def get_swear_words(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() in [\"fuck\", \"shit\", \"bitch\", \"hell\", \"asshole\", \"ass\"]:\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    # putting it all together!\n",
    "    def featurize(self, segments):\n",
    "        feature_dicts = []\n",
    "        docs = self.nlp.pipe(segments)\n",
    "        for doc in docs:\n",
    "            feature_dict = {\n",
    "                \n",
    "                \"n_propernouns\": self.count_propernouns(doc),\n",
    "                \"n_words_before_main_verb\": self.get_n_words_before_main_verb(doc),\n",
    "                \"n_complex_clauses\": self.get_n_complex_clauses(doc),\n",
    "                \"mean_sentiment\": self.get_mean_sentiment(doc),\n",
    "                \"n_pronouns\": self.get_pronouns(doc),\n",
    "                \"n_male_pronouns\": self.get_male_pronouns(doc),\n",
    "                \"n_female_pronouns\": self.get_female_pronouns(doc)\n",
    "                \n",
    "            }\n",
    "            feature_dicts.append(feature_dict)\n",
    "        return feature_dicts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "segment_featurizer = SegmentFeaturizer()\n",
    "class CustomLinguisticFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data):\n",
    "        return segment_featurizer.featurize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"stats\", CustomLinguisticFeatureTransformer()),\n",
    "        (\"dict_vect\", DictVectorizer()),\n",
    "        (\"classifier\", LinearSVC()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pipeline.fit(train_data, train_target)\n",
    "y_pred = manual_pipeline.predict(test_data)\n",
    "crmanual = classification_report(test_target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crmanual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"tfidf\", TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='utf-8', ngram_range=(1, 3), stop_words='english')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"stats\", CustomLinguisticFeatureTransformer()),\n",
    "        (\"dict_vect\", DictVectorizer()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"mean_embeddings\", SpacyVectorTransformer(nlp)),\n",
    "        (\"reduce_dim\", TruncatedSVD(50)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        (\"bow\", bow_pipeline),\n",
    "        (\"manual\", manual_pipeline),\n",
    "    ]\n",
    ")\n",
    "final_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"combined_features\", combined_features),\n",
    "        (\"classifier\",LinearSVC()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline.fit(train_data, train_target)\n",
    "y_pred = final_pipeline.predict(test_data)\n",
    "finalcrmultinomial = classification_report(test_target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalcrmultinomial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(alpha=1.1),\n",
    "    LogisticRegression(C=1e2, random_state=0, fit_intercept=False),\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    combined_features = FeatureUnion(\n",
    "        transformer_list=[\n",
    "            (\"bow\", bow_pipeline),\n",
    "            (\"manual\", manual_pipeline),\n",
    "        ]\n",
    "    )\n",
    "    final_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"combined_features\", combined_features),\n",
    "            (\"classifier\",model),\n",
    "        ]\n",
    "    )\n",
    "    accuracies = cross_val_score(final_pipeline, X, Y, scoring='accuracy', cv=5)\n",
    "    \n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "    \n",
    "    print(model_name + \" done testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_df.to_csv('data/multiplefeaturessklearn.csv')\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.savefig('figures/models/postagclassifier.png')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
