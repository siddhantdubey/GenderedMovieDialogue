{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GenderClassifierCNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6tH2Cejd5mNIP6RRG8NG6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/siddhantdubey/GenderedMovieDialogue/blob/master/GenderClassifierCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLDCKt1YUh8h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a70da689-3b11-4481-d81f-b50f52a18f53"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/Folders/Work/Research/CornellMovie/data/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsGoBhjU0fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.porter import PorterStemmer\n",
        "from gensim.models import Word2Vec\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xbjVrtVfbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = []\n",
        "Y_train = []\n",
        "X_test = []\n",
        "Y_test = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLS7GzJdVk1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(root_dir + 'training_set.txt', encoding=\"charmap\") as training:\n",
        "  for line in training:\n",
        "    line = line.strip()\n",
        "    line_num, chr_id, movie_id, chr_name, chr_gender, line_text, credit = line.split(\"+++$+++\")\n",
        "    if(chr_gender.strip().lower() == \"m\"):\n",
        "      Y_train.append(0)\n",
        "      X_train.append(line_text.strip())\n",
        "    elif(chr_gender.strip().lower() == \"f\"):\n",
        "      Y_train.append(1)\n",
        "      X_train.append(line_text.strip())\n",
        "\n",
        "with open(root_dir + 'test_set.txt', encoding=\"charmap\") as test:\n",
        "  for line in test:\n",
        "    line = line.strip()\n",
        "    line_num, chr_id, movie_id, chr_name, chr_gender, line_text, credit = line.split(\"+++$+++\")\n",
        "    if(chr_gender.strip().lower() == \"m\"):\n",
        "      Y_test.append(0)\n",
        "      X_test.append(line_text.strip())\n",
        "\n",
        "    elif(chr_gender.strip().lower() == \"f\"):\n",
        "      Y_test.append(1)\n",
        "      X_test.append(line_text.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yDQpEQdVmjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X_train + X_test \n",
        "Y = Y_train + Y_test "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYRAB4tWV92A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = []\n",
        "for i in range(len(X)):\n",
        "  if(len(X[i]) > 0):\n",
        "    answer = True \n",
        "  else:\n",
        "    indices.append(i)\n",
        "\n",
        "for index in sorted(indices, reverse=True):\n",
        "    del X[index]\n",
        "    del Y[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pT_ykDeWG2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN075SkaWO1Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "980f50f1-52a1-4256-86e3-f02a89d0d131"
      },
      "source": [
        "data_df['text'] = X\n",
        "data_df['gender'] = Y\n",
        "print(data_df.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  gender\n",
            "0                                       They do not!       1\n",
            "1                                        They do to!       0\n",
            "2                                         I hope so.       1\n",
            "3                                          She okay?       0\n",
            "4                                          Let's go.       1\n",
            "5                                                Wow       0\n",
            "6     Okay -- you're gonna need to learn how to lie.       1\n",
            "7                                                 No       0\n",
            "8  I'm kidding.  You know how sometimes you just ...       1\n",
            "9                   Like my fear of wearing pastels?       1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BzEsihBW5_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_data(top_n = 30000):\n",
        "  top_data_df_male = data_df[data_df['gender'] == 0].head(top_n)\n",
        "  top_data_df_female = data_df[data_df['gender'] == 1].head(top_n)\n",
        "  data_df_small = pd.concat([top_data_df_male, top_data_df_female])\n",
        "  return data_df_small"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkBTLIRhXZmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_data_df_small = get_top_data(top_n=31000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B4-S7hJXhOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "51b9c775-cba1-42ce-922b-d77eb4b9b48c"
      },
      "source": [
        "print(top_data_df_small['gender'].value_counts())\n",
        "print(top_data_df_small.head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1    31000\n",
            "0    31000\n",
            "Name: gender, dtype: int64\n",
            "                                                 text  gender\n",
            "1                                         They do to!       0\n",
            "3                                           She okay?       0\n",
            "5                                                 Wow       0\n",
            "7                                                  No       0\n",
            "10                                    The \"real you\".       0\n",
            "12  I figured you'd get to the good stuff eventually.       0\n",
            "13  Thank God!  If I had to hear one more story ab...       0\n",
            "15                                         What crap?       0\n",
            "17                                              No...       0\n",
            "19                      You always been this selfish?       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlP7shuyXmWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "df3fd1ad-dbdb-40e7-ecb4-6e2d1233c986"
      },
      "source": [
        "top_data_df_small['tokenized_text'] = [simple_preprocess(line, deacc=True) for line in top_data_df_small['text']]\n",
        "print(top_data_df_small['tokenized_text'].head(10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1                                        [they, do, to]\n",
            "3                                           [she, okay]\n",
            "5                                                 [wow]\n",
            "7                                                  [no]\n",
            "10                                     [the, real, you]\n",
            "12    [figured, you, get, to, the, good, stuff, even...\n",
            "13    [thank, god, if, had, to, hear, one, more, sto...\n",
            "15                                         [what, crap]\n",
            "17                                                 [no]\n",
            "19                   [you, always, been, this, selfish]\n",
            "Name: tokenized_text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsMuqJFTX0nX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porter_stemmer = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3x5GiRSYVfx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8c95b7c1-ccd8-445d-86e4-7003baea40bc"
      },
      "source": [
        "top_data_df_small['stemmed_tokens'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in top_data_df_small['tokenized_text']]\n",
        "top_data_df_small['stemmed_tokens'].head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1                                        [thei, do, to]\n",
              "3                                           [she, okai]\n",
              "5                                                 [wow]\n",
              "7                                                  [no]\n",
              "10                                     [the, real, you]\n",
              "12      [figur, you, get, to, the, good, stuff, eventu]\n",
              "13    [thank, god, if, had, to, hear, on, more, stor...\n",
              "15                                         [what, crap]\n",
              "17                                                 [no]\n",
              "19                     [you, alwai, been, thi, selfish]\n",
              "Name: stemmed_tokens, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjdOMnsVYln_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9033e8bd-a649-4878-e0f7-bba60af8acd9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "def split_train_test(top_data_df_small, test_size=0.2, shuffle_state=True):\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(top_data_df_small[['text', 'stemmed_tokens']], \n",
        "                                                        top_data_df_small['gender'], \n",
        "                                                        shuffle=shuffle_state,\n",
        "                                                        test_size=test_size, \n",
        "                                                        random_state=15)\n",
        "    print(\"Value counts for Train genders\")\n",
        "    print(Y_train.value_counts())\n",
        "    print(\"Value counts for Test genders\")\n",
        "    print(Y_test.value_counts())\n",
        "    print(type(X_train))\n",
        "    print(type(Y_train))\n",
        "    X_train = X_train.reset_index()\n",
        "    X_test = X_test.reset_index()\n",
        "    Y_train = Y_train.to_frame()\n",
        "    Y_train = Y_train.reset_index()\n",
        "    Y_test = Y_test.to_frame()\n",
        "    Y_test = Y_test.reset_index()\n",
        "    print(X_train.head())\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# Call the train_test_split\n",
        "X_train, X_test, Y_train, Y_test = split_train_test(top_data_df_small)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Value counts for Train genders\n",
            "0    24842\n",
            "1    24758\n",
            "Name: gender, dtype: int64\n",
            "Value counts for Test genders\n",
            "1    6242\n",
            "0    6158\n",
            "Name: gender, dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "   index  ...                                     stemmed_tokens\n",
            "0  10055  ...  [perfectli, certain, becaus, shall, make, him,...\n",
            "1  36234  ...  [am, small, player, but, if, by, help, you, ca...\n",
            "2  38206  ...         [love, anim, ouch, must, love, anim, rose]\n",
            "3  25549  ...                 [wick, thei, got, your, crew, too]\n",
            "4  47537  ...                                            [peter]\n",
            "\n",
            "[5 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVR92LYNZQB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "805ae3da-92d9-4b0d-9f6b-622892f8f6db"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device available for running: \")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device available for running: \n",
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRnwNJPyZUdn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "13a421b7-ca65-46de-9212-6acbfc764dcd"
      },
      "source": [
        "size = 500\n",
        "window = 3\n",
        "min_count = 1\n",
        "workers = 3\n",
        "sg = 1\n",
        "OUTPUT_FOLDER = root_dir + 'cnnOutputs/'\n",
        "# Function to train word2vec model\n",
        "def make_word2vec_model(top_data_df_small, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
        "    if  padding:\n",
        "        print(len(top_data_df_small))\n",
        "        temp_df = pd.Series(top_data_df_small['stemmed_tokens']).values\n",
        "        temp_df = list(temp_df)\n",
        "        temp_df.append(['pad'])\n",
        "        word2vec_file = OUTPUT_FOLDER + 'models/' + 'word2vec_' + str(size) + '_PAD.model'\n",
        "    else:\n",
        "        temp_df = top_data_df_small['stemmed_tokens']\n",
        "        word2vec_file = OUTPUT_FOLDER + 'models/' + 'word2vec_' + str(size) + '.model'\n",
        "    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
        "\n",
        "    w2v_model.save(word2vec_file)\n",
        "    return w2v_model, word2vec_file\n",
        "\n",
        "# Train Word2vec model\n",
        "w2vmodel, word2vec_file = make_word2vec_model(top_data_df_small, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "62000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thq6Ta38Zq-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_sen_len = top_data_df_small.stemmed_tokens.map(len).max()\n",
        "padding_idx = w2vmodel.wv.vocab['pad'].index\n",
        "def make_word2vec_vector_cnn(sentence):\n",
        "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
        "    i = 0\n",
        "    for word in sentence:\n",
        "        if word not in w2vmodel.wv.vocab:\n",
        "            padded_X[i] = 0\n",
        "            print(word)\n",
        "        else:\n",
        "            padded_X[i] = w2vmodel.wv.vocab[word].index\n",
        "        i += 1\n",
        "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKZut1e8Zukw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_target(label):\n",
        "    if label == 1:\n",
        "        return torch.tensor([0], dtype=torch.long, device=device)\n",
        "    elif label == 0:\n",
        "        return torch.tensor([1], dtype=torch.long, device=device)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KgMwL7pZveo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_SIZE = 500\n",
        "NUM_FILTERS = 2\n",
        "import gensim\n",
        "\n",
        "class CnnTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
        "        super(CnnTextClassifier, self).__init__()\n",
        "        w2vmodel = gensim.models.KeyedVectors.load(OUTPUT_FOLDER + 'models/' + 'word2vec_500_PAD.model')\n",
        "        weights = w2vmodel.wv\n",
        "        # With pretrained embeddings\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n",
        "        # Without pretrained embeddings\n",
        "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n",
        "                                   for window_size in window_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # [B, T, E]\n",
        "\n",
        "        # Apply a convolution + max_pool layer for each window size\n",
        "        x = torch.unsqueeze(x, 1)\n",
        "        xs = []\n",
        "        for conv in self.convs:\n",
        "            x2 = torch.tanh(conv(x))\n",
        "            x2 = torch.squeeze(x2, -1)\n",
        "            x2 = F.max_pool1d(x2, x2.size(2))\n",
        "            xs.append(x2)\n",
        "        x = torch.cat(xs, 2)\n",
        "\n",
        "        # FC\n",
        "        x = x.view(x.size(0), -1)\n",
        "        logits = self.fc(x)\n",
        "\n",
        "        probs = F.softmax(logits, dim = 1)\n",
        "\n",
        "        return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoaPiXdthqci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd2-YHOPaD4P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4eae76c-2437-43c6-9e4b-5ac0762620d4"
      },
      "source": [
        "import time\n",
        "NUM_CLASSES = 2\n",
        "VOCAB_SIZE = len(w2vmodel.wv.vocab)\n",
        "\n",
        "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
        "cnn_model.to(device)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "criterion=nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "num_epochs = 30\n",
        "\n",
        "# Open the file for writing loss\n",
        "loss_file_name = OUTPUT_FOLDER +  'plots/' + 'cnn_class_big_loss_with_padding.csv'\n",
        "f = open(loss_file_name,'w+')\n",
        "f.write('iter, loss')\n",
        "f.write('\\n')\n",
        "losses = []\n",
        "accs = []\n",
        "cnn_model.train()\n",
        "total_time = time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    print(\"Epoch \" + str(epoch + 1))\n",
        "    train_loss = 0\n",
        "    epoch_acc = 0\n",
        "    t= time.time()\n",
        "    for index, row in X_train.iterrows():\n",
        "        # Clearing the accumulated gradients\n",
        "        cnn_model.zero_grad()\n",
        "\n",
        "        # Make the bag of words vector for stemmed tokens \n",
        "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
        "       \n",
        "        # Forward pass to get output\n",
        "        probs = cnn_model(bow_vec)\n",
        "     \n",
        "        # Get the target label\n",
        "        target = make_target(Y_train['gender'][index])\n",
        "   \n",
        "        acc = binary_accuracy(probs, target)\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = loss_function(probs, target)\n",
        "        train_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    # if index == 0:\n",
        "    #     continue\n",
        "    print(\"Epoch ran : \"+ str(epoch+1))\n",
        "    losses.append(train_loss)\n",
        "    accs.append(epoch_acc)\n",
        "    print(\"Loss: \" + str(train_loss/len(X_train)) + '\\n')\n",
        "    print(\"Acc: \" + str(epoch_acc/len(X_train)) + '\\n')\n",
        "    # print(str(train_loss / len(X_train)))\n",
        "    f.write(str((epoch+1)) + \",\" + str(train_loss / len(X_train)))\n",
        "    f.write('\\n')\n",
        "    train_loss = 0\n",
        "    print(\"Time for Epoch: \" + f'time:{time.time()-t:.3f}')\n",
        "torch.save(cnn_model, OUTPUT_FOLDER + 'cnn_big_model_500_with_padding.pth')\n",
        "\n",
        "f.close()\n",
        "print(\"Input vector\")\n",
        "print(bow_vec.cpu().numpy())\n",
        "print(\"Probs\")\n",
        "print(probs)\n",
        "print(torch.argmax(probs, dim=1).cpu().numpy()[0])\n",
        "print(f\"Total training time:{time.time()-total_time:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Epoch ran : 1\n",
            "Loss: 0.6910126071219002\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.548\n",
            "Epoch 2\n",
            "Epoch ran : 2\n",
            "Loss: 0.689455940818354\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:341.777\n",
            "Epoch 3\n",
            "Epoch ran : 3\n",
            "Loss: 0.6879701623044187\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.730\n",
            "Epoch 4\n",
            "Epoch ran : 4\n",
            "Loss: 0.6865106315586356\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:345.374\n",
            "Epoch 5\n",
            "Epoch ran : 5\n",
            "Loss: 0.6865880737225375\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:343.950\n",
            "Epoch 6\n",
            "Epoch ran : 6\n",
            "Loss: 0.6855349385401895\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:345.883\n",
            "Epoch 7\n",
            "Epoch ran : 7\n",
            "Loss: 0.6844313704006133\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.477\n",
            "Epoch 8\n",
            "Epoch ran : 8\n",
            "Loss: 0.6830093969596971\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:348.486\n",
            "Epoch 9\n",
            "Epoch ran : 9\n",
            "Loss: 0.6825892695375989\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.254\n",
            "Epoch 10\n",
            "Epoch ran : 10\n",
            "Loss: 0.6824851126608349\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.729\n",
            "Epoch 11\n",
            "Epoch ran : 11\n",
            "Loss: 0.6814098429187171\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:345.873\n",
            "Epoch 12\n",
            "Epoch ran : 12\n",
            "Loss: 0.68111745068503\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.440\n",
            "Epoch 13\n",
            "Epoch ran : 13\n",
            "Loss: 0.681204010843029\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.503\n",
            "Epoch 14\n",
            "Epoch ran : 14\n",
            "Loss: 0.6804460385910446\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:348.954\n",
            "Epoch 15\n",
            "Epoch ran : 15\n",
            "Loss: 0.6798558300853736\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.281\n",
            "Epoch 16\n",
            "Epoch ran : 16\n",
            "Loss: 0.6797067434304664\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:344.955\n",
            "Epoch 17\n",
            "Epoch ran : 17\n",
            "Loss: 0.6787047811044801\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.323\n",
            "Epoch 18\n",
            "Epoch ran : 18\n",
            "Loss: 0.6783972736339896\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:349.161\n",
            "Epoch 19\n",
            "Epoch ran : 19\n",
            "Loss: 0.678090921587521\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:348.162\n",
            "Epoch 20\n",
            "Epoch ran : 20\n",
            "Loss: 0.6786258617223752\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:349.181\n",
            "Epoch 21\n",
            "Epoch ran : 21\n",
            "Loss: 0.6774603666533385\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.076\n",
            "Epoch 22\n",
            "Epoch ran : 22\n",
            "Loss: 0.6767423950424117\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.330\n",
            "Epoch 23\n",
            "Epoch ran : 23\n",
            "Loss: 0.676915503087784\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.762\n",
            "Epoch 24\n",
            "Epoch ran : 24\n",
            "Loss: 0.6764557315108757\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.359\n",
            "Epoch 25\n",
            "Epoch ran : 25\n",
            "Loss: 0.6764705598522579\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.797\n",
            "Epoch 26\n",
            "Epoch ran : 26\n",
            "Loss: 0.6760708043303701\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.571\n",
            "Epoch 27\n",
            "Epoch ran : 27\n",
            "Loss: 0.675638886807907\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.522\n",
            "Epoch 28\n",
            "Epoch ran : 28\n",
            "Loss: 0.6751622162182485\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.057\n",
            "Epoch 29\n",
            "Epoch ran : 29\n",
            "Loss: 0.6753348113908884\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:347.120\n",
            "Epoch 30\n",
            "Epoch ran : 30\n",
            "Loss: 0.675490222501178\n",
            "\n",
            "Acc: 1.0016935483870968\n",
            "\n",
            "Time for Epoch: time:346.734\n",
            "Input vector\n",
            "[[   0   32 9949    0  974    9    1  637    5  243   10 1011    7 1781\n",
            "     9    1 4672   12 1513  203  353    5  597   13    0   16    1  708\n",
            "    52 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961\n",
            "  3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961 3961]]\n",
            "Probs\n",
            "tensor([[0.6037, 0.3963]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "0\n",
            "Total training time:10405.492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type CnnTextClassifier. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7uwNhjKaZSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "754b8ff3-122f-4794-aabb-03aea24e2c07"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "bow_cnn_predictions = []\n",
        "original_lables_cnn_bow = []\n",
        "cnn_model.eval()\n",
        "loss_df = pd.read_csv(OUTPUT_FOLDER + 'plots/' 'cnn_class_big_loss_with_padding.csv')\n",
        "print(loss_df.columns)\n",
        "test_accs = []\n",
        "# loss_df.plot('loss')\n",
        "with torch.no_grad():\n",
        "    for index, row in X_test.iterrows():\n",
        "        bow_vec = make_word2vec_vector_cnn(row['stemmed_tokens'])\n",
        "        probs = cnn_model(bow_vec)\n",
        "        _, predicted = torch.max(probs.data, 1)\n",
        "        bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n",
        "        original_lables_cnn_bow.append(make_target(Y_test['gender'][index]).cpu().numpy()[0])\n",
        "print(classification_report(original_lables_cnn_bow,bow_cnn_predictions))\n",
        "loss_file_name = OUTPUT_FOLDER +  'plots/' + 'cnn_class_big_loss_with_padding.csv'\n",
        "loss_df = pd.read_csv(loss_file_name)\n",
        "print(loss_df.columns)\n",
        "plt_500_padding_30_epochs = loss_df[' loss'].plot()\n",
        "fig = plt_500_padding_30_epochs.get_figure()\n",
        "fig.savefig(OUTPUT_FOLDER +'plots/' + \"loss_plt_500_padding_30_epochs.pdf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['iter', ' loss'], dtype='object')\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.75      0.63      6242\n",
            "           1       0.57      0.34      0.43      6158\n",
            "\n",
            "    accuracy                           0.55     12400\n",
            "   macro avg       0.56      0.55      0.53     12400\n",
            "weighted avg       0.55      0.55      0.53     12400\n",
            "\n",
            "Index(['iter', ' loss'], dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ihSXmOafaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}