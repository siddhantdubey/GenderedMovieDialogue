{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import cross_val_score, validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/cleanfilmdata.csv')\n",
    "test_df = pd.read_csv('data/tweetdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "class SegmentFeaturizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.future_words = [\"tomorrow\", \"future\", \"futures\"]\n",
    "    \n",
    "    @staticmethod\n",
    "    def count_propernouns(doc):\n",
    "        segment = doc.text.lower().split()\n",
    "        count = 0 \n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.tag_ in ['NNP', 'NNPS']:\n",
    "                count+=1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if(count == 0):\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_words_before_main_verb(doc):\n",
    "        numbers = [0]\n",
    "        for sent in doc.sents:\n",
    "            main = [t for t in sent if t.dep_ == \"ROOT\"][0]\n",
    "            if main.pos_ == \"VERB\":\n",
    "                dist_to_init = main.i - sent[0].i\n",
    "                numbers.append(dist_to_init)\n",
    "        return np.mean(numbers)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_n_complex_clauses(doc):\n",
    "        embedded_elements_count = []\n",
    "        for sent in doc.sents:\n",
    "            n_embedded = len(\n",
    "                [t for t in sent if t.dep_ in {\"ccomp\", \"xcomp\", \"advcl\", \"dative\"}]\n",
    "            )\n",
    "            embedded_elements_count.append(n_embedded)\n",
    "        if len(embedded_elements_count) == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return np.mean(embedded_elements_count)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mean_sentiment(doc):\n",
    "        return doc.sentiment\n",
    "    @staticmethod\n",
    "    def get_pronouns(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"PRON\":\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    @staticmethod\n",
    "    def get_female_pronouns(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.text.lower() in ['her', 'she', 'wife', 'girlfriend']:\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num+=1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    @staticmethod\n",
    "    def get_male_pronouns(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.text.lower() in ['he', 'him', 'husband', 'honey', 'his', 'boyfriend']:\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    def get_swear_words(doc):\n",
    "        count = 0\n",
    "        num = 0\n",
    "        for token in doc:\n",
    "            if token.lemma_.lower() in [\"fuck\", \"shit\", \"bitch\", \"hell\", \"asshole\", \"ass\"]:\n",
    "                count += 1\n",
    "                num += 1\n",
    "            else:\n",
    "                num += 1\n",
    "        if count == 0 or num == 0:\n",
    "            average = 0\n",
    "        else:\n",
    "            average = count/num\n",
    "        return average\n",
    "    # putting it all together!\n",
    "    def featurize(self, segments):\n",
    "        feature_dicts = []\n",
    "        docs = self.nlp.pipe(segments)\n",
    "        for doc in docs:\n",
    "            feature_dict = {\n",
    "                \n",
    "                \"n_propernouns\": self.count_propernouns(doc),\n",
    "                \"n_words_before_main_verb\": self.get_n_words_before_main_verb(doc),\n",
    "                \"n_complex_clauses\": self.get_n_complex_clauses(doc),\n",
    "                \"mean_sentiment\": self.get_mean_sentiment(doc),\n",
    "                \"n_pronouns\": self.get_pronouns(doc),\n",
    "                \"n_male_pronouns\": self.get_male_pronouns(doc),\n",
    "                \"n_female_pronouns\": self.get_female_pronouns(doc)\n",
    "                \n",
    "            }\n",
    "            feature_dicts.append(feature_dict)\n",
    "        return feature_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "segment_featurizer = SegmentFeaturizer()\n",
    "class CustomLinguisticFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data):\n",
    "        return segment_featurizer.featurize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"stats\", CustomLinguisticFeatureTransformer()),\n",
    "        (\"dict_vect\", DictVectorizer()),\n",
    "        (\"classifier\", LinearSVC()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('data/tweetdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "manual_pipeline.fit(train_df['text'], train_df['target'])\n",
    "y_pred = manual_pipeline.predict(tweets['text'])\n",
    "crmanual = classification_report(tweets['target'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
